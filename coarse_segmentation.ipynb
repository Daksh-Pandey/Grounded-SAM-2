{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pynvml\n",
    "\n",
    "def get_least_used_gpu():\n",
    "    \"\"\"\n",
    "    Finds the GPU with the lowest memory utilization and sets it as the device.\n",
    "\n",
    "    Returns:\n",
    "        str: Device to use (\"cuda:x\" for GPU or \"cpu\" if no GPU is available).\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No GPU available. Using CPU.\")\n",
    "        return \"cpu\"\n",
    "\n",
    "    # Initialize NVIDIA Management Library\n",
    "    pynvml.nvmlInit()\n",
    "    device_count = pynvml.nvmlDeviceGetCount()\n",
    "\n",
    "    min_usage = float(\"inf\")\n",
    "    best_gpu = None\n",
    "\n",
    "    for i in range(device_count):\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "        # Calculate memory usage percentage\n",
    "        used_memory = info.used / info.total * 100  # Memory usage in percentage\n",
    "        # print(f\"GPU {i}: {used_memory:.2f}% memory used.\")\n",
    "\n",
    "        # Track the GPU with the lowest memory usage\n",
    "        if used_memory < min_usage:\n",
    "            min_usage = used_memory\n",
    "            best_gpu = i\n",
    "\n",
    "    # Cleanup NVML\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "    if best_gpu is not None:\n",
    "        print(f\"Using GPU {best_gpu} (lowest memory usage: {min_usage:.2f}%).\")\n",
    "        return f\"cuda:{best_gpu}\"\n",
    "    else:\n",
    "        print(\"No suitable GPU found. Using CPU.\")\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU 0 (lowest memory usage: 43.94%).\n"
     ]
    }
   ],
   "source": [
    "from pytorch3d.io import load_objs_as_meshes\n",
    "import os\n",
    "\n",
    "# Setup\n",
    "DEVICE = get_least_used_gpu()\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = \"./data\"\n",
    "obj_filename = os.path.join(DATA_DIR, \"dog_mesh/13466_Canaan_Dog_v1_L3.obj\")\n",
    "\n",
    "# Load obj file\n",
    "mesh = load_objs_as_meshes([obj_filename], device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Failed to load image Python extension: '/home/ip_arul/daksh21036/miniconda3/envs/grounded_sam2/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789115765/work/aten/src/ATen/native/TensorShape.cpp:3609.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "from torchvision.ops import box_convert\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from grounding_dino.groundingdino.util.inference import load_model, load_image, predict\n",
    "\n",
    "\n",
    "SAM2_CHECKPOINT = \"./checkpoints/sam2.1_hiera_large.pt\"\n",
    "SAM2_MODEL_CONFIG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "GROUNDING_DINO_CONFIG = \"grounding_dino/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "GROUNDING_DINO_CHECKPOINT = \"gdino_checkpoints/groundingdino_swint_ogc.pth\"\n",
    "BOX_THRESHOLD = 0.35\n",
    "TEXT_THRESHOLD = 0.25\n",
    "\n",
    "# build SAM2 image predictor\n",
    "sam2_checkpoint = SAM2_CHECKPOINT\n",
    "model_cfg = SAM2_MODEL_CONFIG\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=DEVICE)\n",
    "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "# build grounding dino model\n",
    "grounding_model = load_model(\n",
    "    model_config_path=GROUNDING_DINO_CONFIG, \n",
    "    model_checkpoint_path=GROUNDING_DINO_CHECKPOINT,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "def grounded_sam2(img_path: str, text_prompt: str, img_file_name: str):\n",
    "    # setup the input image and text prompt for SAM 2 and Grounding DINO\n",
    "    # VERY important: text queries need to be lowercased + end with a dot\n",
    "    text = text_prompt\n",
    "\n",
    "    image_source, image = load_image(img_path)\n",
    "\n",
    "    sam2_predictor.set_image(image_source)\n",
    "\n",
    "    # FIXME: figure how does this influence the G-DINO model\n",
    "    # changed bfloat16 to float16\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "\n",
    "        if torch.cuda.get_device_properties(torch.device(DEVICE)).major >= 8:\n",
    "            # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "        boxes, confidences, labels = predict(\n",
    "            model=grounding_model,\n",
    "            image=image,\n",
    "            caption=text,\n",
    "            box_threshold=BOX_THRESHOLD,\n",
    "            text_threshold=TEXT_THRESHOLD,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        # process the box prompt for SAM 2\n",
    "        h, w, _ = image_source.shape\n",
    "        boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "        input_boxes = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "\n",
    "        if input_boxes.size == 0:\n",
    "            print(f\"No objects detected in {img_path}. Skipping this image.\")\n",
    "            return None\n",
    "\n",
    "        masks, scores, logits = sam2_predictor.predict(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            box=input_boxes,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "    \"\"\"\n",
    "    Post-process the output of the model to get the masks, scores, and logits for visualization\n",
    "    \"\"\"\n",
    "    # convert the shape to (n, H, W)\n",
    "    if masks.ndim == 4:\n",
    "        masks = masks.squeeze(1)\n",
    "\n",
    "\n",
    "    confidences = confidences.numpy().tolist()\n",
    "    class_names = labels\n",
    "\n",
    "    class_ids = np.array(list(range(len(class_names))))\n",
    "\n",
    "\n",
    "    labels = [class_name for class_name in class_names]\n",
    "\n",
    "    # labels = [\n",
    "    #     f\"{class_name} {confidence:.2f}\"\n",
    "    #     for class_name, confidence\n",
    "    #     in zip(class_names, confidences)\n",
    "    # ]\n",
    "\n",
    "    \"\"\"\n",
    "    Visualize image with supervision useful API\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    detections = sv.Detections(\n",
    "        xyxy=input_boxes,  # (n, 4)\n",
    "        mask=masks.astype(bool),  # (n, h, w)\n",
    "        class_id=class_ids\n",
    "    )\n",
    "\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    annotated_frame = box_annotator.annotate(scene=img.copy(), detections=detections)\n",
    "\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "    # cv2.imwrite(os.path.join(output_dir, \"groundingdino_annotated_image.jpg\"), annotated_frame)\n",
    "\n",
    "    mask_annotator = sv.MaskAnnotator()\n",
    "    annotated_frame = mask_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    cv2.imwrite(img_file_name, annotated_frame)\n",
    "\n",
    "    return masks, labels   # return sam2 masks and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789115765/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['legs', 'tail', 'tail', 'legs', 'head']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = \"data/dog_mesh_views/view_00.png\"\n",
    "x = grounded_sam2(img_path, text_prompt=\"head. tail. legs.\", img_file_name=\"tmp.png\")\n",
    "arr = x[0]\n",
    "print(np.unique(arr))\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    PerspectiveCameras,\n",
    "    RasterizationSettings,\n",
    "    MeshRasterizer,\n",
    "    MeshRenderer,\n",
    "    SoftPhongShader,\n",
    "    PointLights\n",
    ")\n",
    "from pytorch3d.io import load_objs_as_meshes\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "\n",
    "def calculate_matrix_Xi(\n",
    "    obj_file_path: str,\n",
    "    batch_size: int,\n",
    "    text_prompt: str,\n",
    "    elevs: tuple,\n",
    "    azims: tuple,\n",
    "    save_dir: str,\n",
    "    device: str = DEVICE\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the face-region matrix X_i for a given mesh using grounded_sam2 masks.\n",
    "\n",
    "    Args:\n",
    "        obj_file_path (str): Path to the OBJ file representing the 3D mesh.\n",
    "        batch_size (int): Number of images to render in a batch.\n",
    "        text_prompt (str): Input text for grounded_sam2.\n",
    "        elevs (tuple): Elevation range (start, end) in degrees.\n",
    "        azims (tuple): Azimuth range (start, end) in degrees.\n",
    "        device (str): Device to run the computation on (\"cuda:x\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Matrix X_i of shape (num_faces, num_regions).\n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Load the mesh from the OBJ file\n",
    "    mesh = load_objs_as_meshes([obj_file_path], device=device)\n",
    "    num_faces = mesh.faces_packed().shape[0]\n",
    "\n",
    "    # Parse semantic regions from text_prompt (labels end with '.')\n",
    "    labels = text_prompt.strip().split('.')[:-1]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    num_regions = len(labels)\n",
    "    labels_dict = {}\n",
    "    for idx in range(len(labels)):\n",
    "        labels_dict[labels[idx]] = idx\n",
    "\n",
    "    # Initialize the face-region matrix Xi with zeros\n",
    "    Xi = torch.zeros((num_faces, num_regions), device=device)\n",
    "\n",
    "    # Generate rasterization and rendering settings\n",
    "    raster_settings = RasterizationSettings(\n",
    "        image_size=512,  # Customize based on desired output resolution\n",
    "        blur_radius=0.0,\n",
    "        faces_per_pixel=1,  # Nearest face only\n",
    "        max_faces_per_bin=30000\n",
    "    )\n",
    "\n",
    "    # batched meshes\n",
    "    meshes = mesh.extend(batch_size)\n",
    "\n",
    "    # Create batches of elevation and azimuth angles\n",
    "    elev_angles = torch.linspace(elevs[0], elevs[1], batch_size)\n",
    "    azim_angles = torch.linspace(azims[0], azims[1], batch_size)\n",
    "    # elev_grid, azim_grid = torch.meshgrid(elev_angles, azim_angles, indexing=\"ij\")\n",
    "    # elev_grid, azim_grid = elev_grid.flatten(), azim_grid.flatten()\n",
    "    R, T = look_at_view_transform(60, elev=elev_angles, azim=azim_angles)\n",
    "\n",
    "    # Create batched cameras\n",
    "    cameras = PerspectiveCameras(\n",
    "        device=device,\n",
    "        R=R,\n",
    "        T=T\n",
    "    )\n",
    "\n",
    "    # lights\n",
    "    lights = PointLights(device=device, location=[[0.0, 0.0, -70.0]])\n",
    "\n",
    "    # Initialize the rasterizer and shade\n",
    "    rasterizer = MeshRasterizer(cameras=cameras, raster_settings=raster_settings)\n",
    "    shader = SoftPhongShader(device=device, cameras=cameras, lights=lights)\n",
    "\n",
    "    # Render the images if needed (optional, just to save them)\n",
    "    renderer = MeshRenderer(rasterizer=rasterizer, shader=shader)\n",
    "    images = renderer(meshes, cameras=cameras, lights=lights)\n",
    "    \n",
    "    # Rasterize the mesh to get the fragments\n",
    "    fragments = rasterizer(meshes_world=meshes)\n",
    "\n",
    "    # Now access pix_to_face from fragments\n",
    "    pix_to_face = fragments.pix_to_face[..., 0]  # Shape: (B, H, W)\n",
    "    pix_to_face = pix_to_face % num_faces\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for batch_idx in range(pix_to_face.shape[0]):\n",
    "        # Extract the RGB channels (H, W, 3)\n",
    "        rgb_image = images[batch_idx, ..., :3]  # Take only RGB channels\n",
    "\n",
    "        # Convert to uint8 (0-255 range) if needed\n",
    "        rgb_image = (rgb_image.clamp(0, 1) * 255).byte()\n",
    "\n",
    "        # Convert to PIL image and save\n",
    "        image_pil = ToPILImage()(rgb_image.permute(2, 0, 1).cpu())  # (C, H, W) for ToPILImage\n",
    "        image_path = f\"{save_dir}/view_{batch_idx:02d}.png\"\n",
    "        image_pil.save(image_path)\n",
    "        \n",
    "        # Use grounded_sam2 to get masks and labels for each rendered image\n",
    "        sam2_masks, sam2_labels = grounded_sam2(image_path, text_prompt, \n",
    "                            f\"{save_dir}/segmented_view_{batch_idx:02d}.png\")\n",
    "\n",
    "        # Iterate through all masks in sam2_masks\n",
    "        for mask_idx, label in enumerate(sam2_labels):\n",
    "            # Map the label to its corresponding region index using labels_dict\n",
    "            \n",
    "            if label not in labels_dict:\n",
    "                continue  # Skip unknown labels\n",
    "\n",
    "            region_idx = labels_dict[label]  # Get the column index for this region\n",
    "\n",
    "            # Convert the corresponding 2D mask to a PyTorch tensor\n",
    "            region_mask = torch.tensor(sam2_masks[mask_idx] == 1, device=device)  # Foreground is 1\n",
    "\n",
    "            # Extract valid face indices using the region mask\n",
    "            valid_face_indices = pix_to_face[batch_idx][region_mask]\n",
    "            valid_face_indices = valid_face_indices[valid_face_indices >= 0]  # Ignore background (-1)\n",
    "\n",
    "            if valid_face_indices.numel() > 0:  # If there are valid faces\n",
    "                # Count occurrences of each face\n",
    "                face_counts = torch.bincount(valid_face_indices, minlength=num_faces)\n",
    "\n",
    "                # Update the face-region matrix Xi\n",
    "                Xi[:, region_idx] += face_counts\n",
    "\n",
    "    return Xi.cpu().numpy(), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_to_label_mapping(Xi: np.ndarray)->np.ndarray:\n",
    "    return np.argmax(Xi, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_colored_mesh(obj_file_path: str, region_indices: np.ndarray,\n",
    "                                      region_labels, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Plot a PyTorch3D Meshes object with faces colored based on their region labels.\n",
    "\n",
    "    Args:\n",
    "        obj_file_path (str): Path to the OBJ file representing the 3D mesh.\n",
    "        region_indices (np.ndarray): Array of shape (num_faces,) containing the region index for each face.\n",
    "        region_labels (list): List of region labels corresponding to region indices.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays the plot.\n",
    "    \"\"\"\n",
    "    # Load the mesh from the OBJ file\n",
    "    mesh = load_objs_as_meshes([obj_file_path], device=device)\n",
    "    # Extract vertices and faces from the Meshes object\n",
    "    vertices = mesh.verts_packed().cpu().numpy()  # Shape: (num_vertices, 3)\n",
    "    faces = mesh.faces_packed().cpu().numpy()  # Shape: (num_faces, 3)\n",
    "\n",
    "    # Generate unique colors for each region\n",
    "    num_regions = len(region_labels)\n",
    "    colors = [\n",
    "        f\"rgb({np.random.randint(0,255)},{np.random.randint(0,255)},{np.random.randint(0,255)})\"\n",
    "        for _ in range(num_regions)\n",
    "    ]\n",
    "\n",
    "    # Map each face to its region color\n",
    "    face_colors = [colors[region_idx] for region_idx in region_indices]\n",
    "\n",
    "    # Create the 3D triangular mesh plot\n",
    "    fig = go.Figure(data=[\n",
    "        go.Mesh3d(\n",
    "            x=vertices[:, 0],\n",
    "            y=vertices[:, 1],\n",
    "            z=vertices[:, 2],\n",
    "            i=faces[:, 0],\n",
    "            j=faces[:, 1],\n",
    "            k=faces[:, 2],\n",
    "            facecolor=face_colors,  # Assign colors to faces\n",
    "            opacity=1.0\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # Add a legend for the regions\n",
    "    for region_idx, label in enumerate(region_labels):\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=[None], y=[None], z=[None],  # Dummy trace for legend\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=10, color=colors[region_idx]),\n",
    "                name=label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Set the layout for better visualization\n",
    "    fig.update_layout(\n",
    "        title=\"Colored Mesh by Region\",\n",
    "        scene=dict(\n",
    "            xaxis_title=\"X\",\n",
    "            yaxis_title=\"Y\",\n",
    "            zaxis_title=\"Z\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN THIS\n",
    "\n",
    "text_prompt = \"head. tail. legs. eyes. ears. chest.\"\n",
    "obj_file_path = \"data/dog_mesh/13466_Canaan_Dog_v1_L3.obj\"\n",
    "save_dir = \"./data/dog_mesh_views\"\n",
    "batch_size = 30\n",
    "\n",
    "Xi, labels = calculate_matrix_Xi(obj_file_path=obj_file_path,\n",
    "                    batch_size=batch_size,\n",
    "                    text_prompt=text_prompt,\n",
    "                    elevs=(30, 40),\n",
    "                    azims=(-180, 180),\n",
    "                    save_dir=save_dir,\n",
    "                    device=DEVICE)\n",
    "print(Xi.shape)\n",
    "face_to_label = get_face_to_label_mapping(Xi)\n",
    "plot_colored_mesh(obj_file_path, face_to_label, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grounded_sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
